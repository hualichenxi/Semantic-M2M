
%% bare_conf.tex
%% V1.3
%% 2007/01/11
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.7 or later) with an IEEE conference paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE!
%% User assumes all risk.
%% In no event shall IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%
%% File list of work: IEEEtran.cls, IEEEtran_HOWTO.pdf, bare_adv.tex,
%%                    bare_conf.tex, bare_jrnl.tex, bare_jrnl_compsoc.tex
%%*************************************************************************

% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. IEEE's font choices can trigger bugs that do  ***
% *** not appear when using other class files.                            ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



% Note that the a4paper option is mainly intended so that authors in
% countries using A4 can easily print to A4 and see how their papers will
% look in print - the typesetting of the document will not typically be
% affected with changes in paper size (but the bottom and side margins will).
% Use the testflow package mentioned above to verify correct handling of
% both paper sizes by the user's LaTeX system.
%
% Also note that the "draftcls" or "draftclsnofoot", not "draft", option
% should be used if it is desired that the figures are to be displayed in
% draft mode.
%
\documentclass[10pt, conference, compsocconf]{IEEEtran}
\usepackage{algorithm}
%\usepackage{algorithmic}
\usepackage{algpseudocode}
\usepackage{amsmath}
\usepackage{graphics}
\usepackage{epsfig}
\usepackage{graphicx}

\usepackage{cite}
\usepackage{subfigure}
\usepackage{float}

\usepackage{epstopdf}
\usepackage{enumerate}
\usepackage{amsmath,amsthm,amssymb,amsfonts}
\makeatletter
\thm@headfont{\sc}
\makeatother
\newtheorem{theorem}{Property}
% Add the compsocconf option for Computer Society conferences.
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[conference]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/tex-archive/macros/latex/contrib/oberdiek/
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 4.0 (2003-05-27) and later if using hyperref.sty. cite.sty does
% not currently provide for hyperlinked citations.
% The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/cite/
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/graphics/
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found as epslatex.ps or
% epslatex.pdf at: http://www.ctan.org/tex-archive/info/
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage[cmex10]{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics. If using
% it, be sure to load this package with the cmex10 option to ensure that
% only type 1 fonts will utilized at all point sizes. Without this option,
% it is possible that some math symbols, particularly those within
% footnotes, will be rendered in bitmap form which will result in a
% document that can not be IEEE Xplore compliant!
%
% Also, note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/amslatex/math/





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithms/
% There is also a support site at:
% http://algorithms.berlios.de/index.html
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithmicx/




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/tools/


%\usepackage{mdwmath}
%\usepackage{mdwtab}
% Also highly recommended is Mark Wooding's extremely powerful MDW tools,
% especially mdwmath.sty and mdwtab.sty which are used to format equations
% and tables, respectively. The MDWtools set is already installed on most
% LaTeX systems. The lastest version and documentation is available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/mdwtools/


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.


%\usepackage{eqparbox}
% Also of notable interest is Scott Pakin's eqparbox package for creating
% (automatically sized) equal width boxes - aka "natural width parboxes".
% Available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/eqparbox/





% *** SUBFIGURE PACKAGES ***
%\usepackage[tight,footnotesize]{subfigure}
% subfigure.sty was written by Steven Douglas Cochran. This package makes it
% easy to put subfigures in your figures. e.g., "Figure 1a and 1b". For IEEE
% work, it is a good idea to load it with the tight package option to reduce
% the amount of white space around the subfigures. subfigure.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at:
% http://www.ctan.org/tex-archive/obsolete/macros/latex/contrib/subfigure/
% subfigure.sty has been superceeded by subfig.sty.



%\usepackage[caption=false]{caption}
%\usepackage[font=footnotesize]{subfig}
% subfig.sty, also written by Steven Douglas Cochran, is the modern
% replacement for subfigure.sty. However, subfig.sty requires and
% automatically loads Axel Sommerfeldt's caption.sty which will override
% IEEEtran.cls handling of captions and this will result in nonIEEE style
% figure/table captions. To prevent this problem, be sure and preload
% caption.sty with its "caption=false" package option. This is will preserve
% IEEEtran.cls handing of captions. Version 1.3 (2005/06/28) and later
% (recommended due to many improvements over 1.2) of subfig.sty supports
% the caption=false option directly:
%\usepackage[caption=false,font=footnotesize]{subfig}
%
% The latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/subfig/
% The latest version and documentation of caption.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/caption/




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure. The latest version and documentation can be found at:
% http://www.ctan.org/tex-archive/macros/latex/base/



%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/sttools/
% Documentation is contained in the stfloats.sty comments as well as in the
% presfull.pdf file. Do not use the stfloats baselinefloat ability as IEEE
% does not allow \baselineskip to stretch. Authors submitting work to the
% IEEE should note that IEEE rarely uses double column equations and
% that authors should try to avoid such use. Do not be tempted to use the
% cuted.sty or midfloat.sty packages (also by Sigitas Tolusis) as IEEE does
% not format its papers in such ways.





% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/misc/
% Read the url.sty source comments for usage information. Basically,
% \url{my_url_here}.





% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}

\bibliographystyle{plain}

\addtolength\itemsep{0em}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{SparkRDF: Elastic Discreted RDF Graph Processing Engine With Distributed Memory}


% author names and affiliations
% use a multiple column layout for up to two different
% affiliations

\author{\IEEEauthorblockN{Xi Chen}
\IEEEauthorblockA{Department of Computer Science\\
Zhejiang University\\
Hangzhou, China\\
xichen@zju.edu.cn}
\and
\IEEEauthorblockN{Huajun Chen}
\IEEEauthorblockA{Department of Computer Science\\
Zhejiang University\\
Hangzhou, China\\
huajunsir@zju.edu.cn}
\and
\IEEEauthorblockN{Peiqin Gu}
\IEEEauthorblockA{Department of Computer Science\\
Zhejiang University\\
Hangzhou, China\\
peiqingu@zju.edu.cn}
}

% conference papers do not typically use \thanks and this command
% is locked out in conference mode. If really needed, such as for
% the acknowledgment of grants, issue a \IEEEoverridecommandlockouts
% after \documentclass

% for over three affiliations, or if they all won't fit within the width
% of the page, use this alternative format:
%
%\author{\IEEEauthorblockN{Michael Shell\IEEEauthorrefmark{1},
%Homer Simpson\IEEEauthorrefmark{2},
%James Kirk\IEEEauthorrefmark{3},
%Montgomery Scott\IEEEauthorrefmark{3} and
%Eldon Tyrell\IEEEauthorrefmark{4}}
%\IEEEauthorblockA{\IEEEauthorrefmark{1}School of Electrical and Computer Engineering\\
%Georgia Institute of Technology,
%Atlanta, Georgia 30332--0250\\ Email: see http://www.michaelshell.org/contact.html}
%\IEEEauthorblockA{\IEEEauthorrefmark{2}Twentieth Century Fox, Springfield, USA\\
%Email: homer@thesimpsons.com}
%\IEEEauthorblockA{\IEEEauthorrefmark{3}Starfleet Academy, San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212, Fax: (888) 555--1212}
%\IEEEauthorblockA{\IEEEauthorrefmark{4}Tyrell Inc., 123 Replicant Street, Los Angeles, California 90210--4321}}




% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle


\begin{abstract}
With the explosive growth of semantic data on the Web over the past years, many large-scale RDF knowledge bases with billions of facts are generating. This poses
significant challenges for the storage and retrieval of huge RDF graphs. Current systems and methods still can not process web scale RDF data effectively. In this paper, we introduce the SparkRDF, an elastic discreted semantic graph processing engine with distributed memory. To reduce the high I/O and communication costs for distributed processing platforms such as Hadoop, SparkRDF implements SPARQL query based on SPARK, a novel in-memory distributed computing framework for big data processing. All the intermediate join results are cached in the distributed memory to accelerate the process of next join.
Instead of building a traditional massive set of permutations of (S,P,O) indexes which causes large space and memory overhead,
we store the RDF data in the elastic discreted subgraphs based on the predicates and classes of the ontology. For SPARQL query optimization, SparkRDF's query processor dynamically generates an optimal execution plan for join queries, leading to effective reduction on the size of intermediate results, the number of joins and the cost of communication. Our extensive evaluation demonstrates that SparkRDF can efficiently answers non-selective joins faster than both current state-of-the-art distributed and centralized stores,
%while being only tenths of a second slower in simple queries,
scaling linearly to the amount of data and machines.
%In our experiment, we focus on the problem that looks for the relationship between Chinese Medicine and Western Medicine. The experimental results show that our approach achieves high performance, accuracy and scalability.


%Moreover, biomedical data is interlinked with complex semantic associations. Every biomedical entity links to many other biomedical entities with association relationships. When we are
%faced with such massive and complicated data, biomedical data integration and analysis become a challenge. In this paper, we utilize the OWL (Ontology Web Language) reasoning technology with distributed computing architecture MapReduce to deal with the problem of massive biomedical data analysis. OWL reasoning method is ideally suitable for problems involved complex semantic associations because it is able to infer logical consequences based on a set of asserted rules or axioms.
%MapReduce framework is used to solve the problem of memory overflow and low efficiency.
%In our experiment, we focus on the problem that looks for the relationship between Chinese Medicine and Western Medicine. The experimental results show that our approach achieves high performance, accuracy and scalability.
\end{abstract}

\begin{IEEEkeywords}
Large Semantic Graph, RDF, SPARQL, SPARK, Distributed memory.
\end{IEEEkeywords}


% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\section{Introduction}
% no \IEEEPARstart

% You must have at least 2 lines in the paragraph with the drop letter
% (should never be an issue)

With the development of Semantic technologies and Web 3.0,
the amount of Semantic Web data represented by the Resource Description Framework (RDF) is increasing rapidly. Commercial search engines including Google and Bing add the semantics of their web contents by RDFa to improve the query accuracy. At the same time, a growing number of organizations or Community driven projects
are constructing large knowledge bases with billions of facts in many domains including bioinformatics, life sciences, social networks and so on. For example, The UniProt Knowledge base contains . The DBpedia and Probase also have billions of facts in RDF format. The Linked Open Data Project even announces 52 billion triples were published by 2012.

RDF data consists of triples where each triple presents a relationship between its subject (S) and object (O), the name of the relationship is given by the predicate (P), and the triple is represented as (S P O). The collections of triples form a labeled directed graph. SPARQL is the query language for RDF graph by providing subgraph matching queries. How to manage the large-scale RDF graph imposes technical challenges to the conventional storage layout, indexing and query processing. There are many researches focusing on the RDF data management. We can summarize these work as the following three categories:1)single-machine systems: SW-Store, Hexastore, TripleBit, and RDF-3x represent the typical examples. Obviously, the centralized solutions are vulnerable to the growth of the data size. 2)MapReduce-based distributed RDF systems: HadoopRDF, H2RDF and Shard. As the SPARQL graph pattern query needs to be implemented by multiple iterations, it leads to the large amount of disk i/o and communications generated. 3)Distributed in-memory RDF systems: Trinity.RDF.The main drawback of this system is that its performance is bound by the main memory capacity of the cluster, as the whole set of triples needs to be loaded in main memory. This is not a scalable approach, as it needs very strict cluster and memory limitations.

The key of processing large RDF graph is to construct efficient execution plans that can be adapted for the storage scheme and the optimization of SPARQL queries. SPARQL queries are made up of several Basic Graph Patterns(BGP), with the capabilities of filtering RDF triples and binding variables to values. When two triple patterns share common variables, the results of two triple patterns need to be joined. It will contain many iterative joins. Querying these huge graphs involves several stages: store the RDF data and creat indexes, read necessary data inside memory, compute binding values for BGP, implement joins over intermediate results, and build the final results of the query. Hence, a desired query processing system should meet the following requirements(i) reduce the data needed to be loaded in memory (ii)reduce the intermediate results (iii)reduce the number of join(iv)reduce the i/o and communication cost of intermediate results.

%这里对大规模语义数据处理应该有两个挑战：1.数据量大2.查询涉及到多次迭代处理
%是不是还有一个挑战是sparql是迭代型操作，正适合于spark的处理

In this paper, we introduce SparkRDF, an elastic discreted RDF graph processing system with distributed memory, which is capable of handling web scale RDF data (billion or enen more). Unlike existing systems that use a set of permutations of (S,P,O) indexes to manage RDF, SparkRDF splits the RDF graph into multiple discreted RDF subgraphs based on the nature of semantic graph: type and relationship. Thus it creates 5 kinds of elastic indexes with different grains (T,P,TP,PT,TPT) to cater for diverse BGP queries. These discreted index files on demand are modeled as a collection of objects partitioned across machines on Spark framework, a fast in-memory cluster computing system which is quite suitable for large-scale iterative computing jobs. Generated intermediate results remain in the distributed memory to support further joins. Based on the data model, several query optimization strategies are made to improve query efficiency including BGP merging, join order making and data prepartitioning.

%\begin{figure}
 % \centering
  %\includegraphics[width=3.5in,bb=0 0 464 346]{protein.jpg}
  %\caption{Linked Knowledge Graph Related to Protein}\label{}
%\end{figure}

\begin{figure*}[htbp]
\centering
\includegraphics[width=1\textwidth,bb=0 0 556 297]{architecture.pdf}
\caption{Biological Conceptional Network and Corresponding Reasoning Property Chains}
\label{}
\end{figure*}


We summarize the main contributions of this work as follows:
\begin{itemize}
\item We design a storage scheme to split RDF graph into elastic discreted subgraphs, which allows to index and query large RDF datasets at a low price to avoid the overhead of memory.
\item We devise a cost model and several optimization strategies for RDF query processing with distributed memory based on the characteristics of Spark and Sparql. These approaches ensure high performance on web scale RDF data.
\item We perform a experimental evaluation of our system. Results show that SparkRDF can be faster than both current state-of-the-art
distributed and centralized stores for some query(complex/non-selective), while being only ... slower in selective onws. Meanwhile, it also can scale linearly to the amount of data and
machines.
\end{itemize}


The remaining of this paper is organized as follows. Section 2 outlines related work and gives a brief introduction of Spark distributed computing platform. In Section 3
, we introduce the architecture of the \textit{SparkRDF} system. Section 4 describes how we model and split big RDF graph. Section 5 gives the distributed SPARQL query processing techniques. In Section 6, we present the results of our experiments. Finally, we conclude and discuss the future work in Section 7.

\section{RELATED WORK and SPARK}

\subsection{RDF Processing Systems}
\subsection{SPARK Computing Platform}
\section{System Architecture}
Figure 2 shows the architecture of SparkRDF. The SparkRDF consists of five modules: data preprocessing module, graph splitting module, distributed storage module, query parser module and distributed join module in memory.

The data preprocessing module is responsible for unifying the format of input data by converting XML-like RDF data to N-Triples. Then the big RDF graph is splitted into several elastic discreted subgraphs by
graph splitting module. The distributed storage module subsequently persists the indexes into HDFS. Query parser module takes the SPARQL query from the user, dynamically generates the query plan and required input list for next step. Based on the input list, corresponding indexes are loaded into the distributed memory for further implementing iterative joins in memory as the query plan requested. At last, the query results are returned to the user.
\section{Data Modeling}

In Spark, RDD is a distributed memory abstraction that lets programmers perform in-memory computations on large clusters. All the input data needs to be modeled as RDD to support further operations. However, the size of memory even on large clusters is far from the amount of massive RDF graph data. The RDF graph should be elaborately splitted into small parts to fit the cost of memory and query. Conventional systems, which use a set of permutations of (S,P,O) indexes, are not applicable since they need to query different kinds of big index files based on the type of BGP. It will easily lead to the overhead of memory. Vertical partitioning solution splits RDF graph by predicates. But its performance is drastically reduced as the data set size is increased. This is because RDF graph has a severe data skew based on different predicates, especially for some large datasets. For example, (make a statistics based on predicates.)It will also suffer from the overhead of memory and low efficiency when handling big predicate files. The predicate files need a more elastic and finer grained splitting scheme.

In fact, RDF data is created by the specific ontology, which represents knowledge as a hierachy of concepts within a domain using a shared vocabulary to denote the types, properties and interrelationships of those concepts. So in nature, semantic RDF graph data is organised by class and predicate. On the other hand, the SPARQL query, expressing a subgraph of RDF graph with several BGPs, is also closely related to entity type and predicate. Table 1 shows the percentage of type and predicate in common semantic datasets and corresponding SPARQL queries. So the information about class and predicate should be considered as the key to split RDF graph.

In this section, we describe how we model the RDF graph as elastic discreted subgraphs based on RDF Graph Splitter.
%这一段要说明数据建模的原则：为了减少搜索空间和内存开销，需要构建一个灵活的索引策略

 %In nature, a predicate represents the binary relationship between two specific classes. Thus




\begin{table}[H]
\caption{Statistics of queries.}
\begin{tabular}{|c|c|c|}
\hline
\hline
\textbf{Dataset} & \textbf{Percentage of Predicate} &  \textbf{Percentage of Type} \\\hline
LUBM &  &  \\\hline
BTC-10 &  & \\\hline

\end{tabular}
\end{table}


\begin{table}[H]
\caption{Evaluation of }
\setlength{\abovecaptionskip}{2pt}
\setlength{\belowcaptionskip}{-2pt}
\label{table_example}
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Dataset} & \textbf{Predicates} &  \textbf{Classes}&  \textbf{Triples} &  \textbf{Percentage of Type} \\\hline
LUBM-10000 &  & &  &  \\\hline
LUBM-20000 &  & &  &  \\\hline
\end{tabular}
\end{center}
\end{table}




\subsection{Type indexes and Predicate indexes}
In the first step, we construct the type indexes and predicate indexes by Predicate Splitter (SP). Specifically, we split the RDF graph by two ways. For the triples, whose predicate is not \textbf{rdf:type}, we extract the triples's subject and object into corresponding predicate index files(predicate being as the file name). Thus the triples sharing the same predicate are grouped together to accelerate the SPARQL queries that have a binding predicate. For other triples( predicate is \textbf{rdf:type}), we divide them into small class files based on the triple's object representing a specific class. The instances belonging to the same class are stored in corresponding type index files(object being as the file name). This further reduces the amount of storage space because the type index files only contain these triples' subject.


\subsection{Type\&Predicate index}
When the RDF graph is splitted into discreted type indexes and predicate indexes, it enables us to select precise index files to reduce search time and space. But we actually only use the predicate information of a BGP, while other information such as its subject and object is missed. What's more, some special index files still remain quite large. So a finer-grained partitioning scheme should be made to further cut down the search space.

A predicate represents the binary relationship between subject and object, whose type information is usually given in a SPARQL query(Statistics). Thus we can divide the predicate files according to the type of the subjects and objects. The type information of the subject and object in a triple is available in type indexes (we cache the type information in last step to support fast indexing). Take the triple (Student0, memberOf, Department0 ) for example, as its subject and object belong to Student and Department respectively, tuple (Student0, Department0) move into three different kinds of index files named as Student\_memberOf(TP index), memberOf\_Department(PT index), Student\_memberOf\_Department(TPT index). The flexible index method makes SparkRDF capable of processing different SPARQL queries efficiently by selecting the minimum input based on these elastic discreted index files.


%Similarly,
%For some common predicates such as \textbf{emailAddress}, which are shared by many different types of subjects
\subsection{Example Data}

To achieve a better performance, we replace all literals in RDF triples by a long id value.
Table  shows the number and size of index files related to LUBM query 8( or 2) for data from 1,000 universities(the compressed total size is 5.659G). From the table, we can see the size of index files vary
A typical example is \textbf{emailAddress} file, For SparkRDF, it only loads three index files(the total size is ) into memory compared to vertical partitioning(), which achieves great memory space gain and high efficiency.
Table 2 gives the some sample data for .
\subsection{Resident Distributed Semantic Graph}
SparkRDF models the index files using an unified concept called RDSG(Resilient Discreted Semantic Graph). It is the basic abstraction of index files in the distributed memory. Based on the API of Spark, we provide the following three main operators with which we implement query processing.

\begin{enumerate}[(i)]
\item \textbf{RDSG\_Gen:} The operator creates a RDSG given the name of index file i.e., RDSG\_Gen(Department) returns all the department instances,
RDSG\_Gen(Department\_subOrganizationOf) returns all departments with corresponding parent organizations(RDSG1),
RDSG\_Gen(Student\_memberOf\_Department) returns all students with their department(RDSG2).
\item \textbf{RDSG\_Filter:} The operator filters RDSG based on the subject or object constrains given in a BGP i.e., RDSG\_Filter(RDSG1,"University0", flag) filters the departments whose parent organization is not "University0". Flag is a parameter that specifies whether the constrain is subject or object.
\item \textbf{RDSG\_Join:} Generally, a query is comprised of multiple subpatterns that are linked together to form the final graph pattern. The operator implements join operation on a set of RDSG and takes in as parameters, the labels of the two types of RDSG to be joined and a join condition e.g. RDSG\_Join(RDSG1, RDSG2, RDSG1.S=RDSG2.O) returns the result of the subpattern consisting of BGP3 and BGP4
\end{enumerate}




\section{Query Processing}
In this section, we discuss how we process SPARQL queries in SparkRDF.
%这里需要画一个图
\begin{itemize}
\item BGP$_1$: (?X rdf:type ub:Student)
\item BGP$_2$: (?Y rdf:type ub:Department)
\item BGP$_3$: (?W rdf:type ub:Professor)
\item BGP$_4$: (?X ub:membeOf ?Y)
\item BGP$_5$: (?Y ub:subOrganizationOf University0)
\item BGP$_6$: (?X ub:emailAddress ?Z)
\item BGP$_7$: (?X rdf:hasAdvisor ?W)
\end{itemize}

\subsection{Overview}
We represent a SPARQL query by a query graph Q, which is made up of multiple triple patterns denoted as BGP. Figure shows the query graph corresponding to Example 1.

With Q defined, the problem of SPARQL query processing can be transformed to the problem of subgraph matching. The solution is conducted as follows: Firstly decompose Q into an ordered sequence of BGP: BGP$_{1}$,...,BGP$_{n}$. Then we compute the matches for each BGP$_{i}$ based on RDSG and corresponding operators, and the matched intermediate results are used to find the matches for BGP$_{i+1}$ by implementing joins on the shared variable of the two BGPs. At last, the remaining results are the final answer of the query graph.
\subsection{Cost Estimation}
We run BGP-based Spark jobs to answer a SPARQL query. Every job mainly contains three tasks, which are reading, matching and joining.
The cost of reading and matching BGP$_{i}$, which are both in positive correlation with the size of index files, are denoted as $|$Read(BGP$_{i}$)$|$ and $|$Match(BGP$_{i}$)$|$. The cost of joining two RDSG is composed of shuffle communication cost in distributed environment and join computation cost in single node: Shuffle(RDSG1,RDSG2), Computation(RDSG1,RDSG2), both of which are roughly proportional to the size of RDSG. The cost parameters are assigned a different weight according to the bandwidth of disk, network and memory.
%is estimated as



\begin{equation}
\begin{aligned}
COST= &\sum_{i=1}^n (\mu|Read(BGP_i)|+\nu|Match(BGP_i)|)+\\
&\sum_{i=2}^n Join(Result_{i-1},Match(BGP_i))\\
=&\sum_{i=2}^n Job_i+\mu|Read(BGP_1)|+\nu|Match(BGP_1)|
\end{aligned}
\end{equation}


\begin{equation}
\begin{aligned}
Job_i=&\mu|Read(BGP_i)|+\nu|Match(BGP_i)|+\\
&Join(Result_{i-1},Match(BGP_i)
\end{aligned}
\end{equation}

\begin{equation}
\begin{aligned}
|Join(RDSG_1,RDSG_2)|=&\lambda|Shuffle(RDSG_1,RDSG_2)|+\\
&\omega|Computation(RDSG_1,RDSG_2)|
\end{aligned}
\end{equation}


\begin{equation}
Result_i=
\left\{
\begin{array}
{l@{\quad:\quad}l}
Join(Result_{i-1},Match(BGP_i)) & $i$>=2 \\
Match(BGP_i) & $i$=1
\end{array}
\right.
\end{equation}

Where,

\   \   \   \   Read(BGP$_{i}$)=the cost of loading selected input index files into RDSG.

\   \   \   \   Match(BGP$_{i}$)=the cost for single triple pattern matching BGP$_{i}$.

\   \   \   \   Result$_{i}$=the intermediate results for BGP$_{1}$, ... ,BGP$_{i}$.

\   \   \   \   Join(RDSG1,RDSG2)=the cost of joining RDSG1 and RDSG2.

\   \   \   \   Shuffle(RDSG1,RDSG2)=the cost of moving data in distributed environment.

\   \   \   \   Computation(RDSG1,RDSG2)=the cost of implementing join operation in single node.



Equation(1) and Equation(2) estimate the total cost of processing a query. It is the summation of
the (n-1) individual job costs and the reading and matching cost of the first job(n is the number of BGP in the query). We can reduce the total query cost by two ways: cut down the number of jobs and the cost of every job.
The former can be achieved by the optimization strategy called merging BGP(see section ). The latter can be implemented
by optimizing join order and data partitioning(Equation 3). Equation (4) shows the cost of implementing join operation
in the SparkRDF. Equation(5) calculates the intermediate results for multiple iterative Sparql query.




\subsection{Query Optimization}
\emph{Definition 1:}\textbf{ Recilient Discreted Semantic Graph (RDSG)}.
\emph{Definition 2:}\textbf{ Selectivity Score (Score)}.
\emph{Definition 3:}\textbf{ Candidate Joining Variable.} A variable that exists in two or more BGPs.
\emph{Definition 4:}\textbf{ Joining Variable}.
\emph{Definition 5:}\textbf{ }.

\begin{theorem}

\end{theorem}
\begin{theorem}
\end{theorem}
\begin{theorem}
\end{theorem}
\subsection{Overview}
%\begin{figure}
%  \centering
%  \includegraphics[width=5.1in,bb=0 0 840 518]{architecture.jpg}
%  \caption{The overall architecture of BioTCM-SE}\label{}
%\end{figure}

%
%\begin{figure}
%  \centering
%  \includegraphics[width=5.1in,bb=0 0 515 384]{association.jpg}
%  \caption{The basic associations that are concerned and observed between Traditional
%Chinese Medicine and modern biology}\label{}
%\end{figure}

%The background unified data model of \textit{BioTCM-SE} is an ontology that is supposed to define an explicit specification of the conceptualization of the abstract view of the integrated modern biology and Traditional Chinese Medicine.



%\begin{center}
%\textit{http://www.geneontology.org/go\#GO:0006392}\\
%\textit{http://purl.uniprot.org/go/0006392}\\
%\textit{urn:lsid:geneontology.org.lsid.biopathways.org:go:0006392}\\
%\end{center}
%based on the \texttt{Unified BioTCM Ontology}



%\begin{table}[!hbp]
%\centering
%\begin{tabular}{|c|c|c|}
%\hline
%\hline
%\textbf{Graph} & \textbf{No.Triples} &  \textbf{Size (MB)} \\\hline
%http://www.biotcm.org/TCM\_hierarchy & 177, 960 & 7.4 \\\hline
%http://www.biotcm.org/Reactome & 1, 082, 499 & 110.7 \\\hline
%
%\end{tabular}
%\centering
%\caption{Major data sets that have been integrated in BioTCM-SE.}
%\end{table}




%\begin{figure}
%  \centering
%  \includegraphics[width=5.1in,bb=0 0 867 362]{sparql.jpg}
%  \caption{The workflow for searching possible herbs related to gene Interleukin 6 (IL6)}\label{}
%\end{figure}

\section{Results}

%Every oval maps to a data set which is marked up by a number representing the number of triples in the dataset.
%\begin{figure}
%  \centering
%  \includegraphics[width=5.1in,bb=0 0 1094 344]{knowledge_graph.jpg}
%  \caption{The big linked biomedical knowledge graph of BioTCM-SE}\label{}
%\end{figure}
%
%\begin{figure}
%\centering
%\includegraphics[width=5.2in]{site.jpg}
%\caption{The screenshots of the \textit{BioTCM-SE} web application, including pages for the main access point, Bio for TCM search, TCM for Bio search, terminology search, data sets browse, and a graph visualization for the related herbs of the gene \textit{IL6} and disease \textit{Breast Neoplasm}.}
%\end{figure}




\section{Discussion and Conclusion}

%\cite{wild2012systems}
%
%\cite{ruttenberg2007advancing}


%\bibliographystyle{elsarticle-num}
%\bibliography{Sparkrdf}

%% Authors are advised to submit their bibtex database files. They are
%% requested to list a bibtex style file in the manuscript if they do
%% not want to use elsarticle-num.bst.

%% References without bibTeX database:

% \begin{thebibliography}{00}

%% \bibitem must have the following form:
%%   \bibitem{key}...
%%

% \bibitem{}

% \end{thebibliography}


\end{document}
% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex,
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation Results}
%\label{fig_sim}
%\end{figure}

% Note that IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command, the
% \label for the overall figure must come after \caption.
% \hfil must be used as a separator to get equal spacing.
% The subfigure.sty package works much the same way, except \subfigure is
% used instead of \subfloat.
%
%\begin{figure*}[!t]
%\centerline{\subfloat[Case I]\includegraphics[width=2.5in]{subfigcase1}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{subfigcase2}%
%\label{fig_second_case}}}
%\caption{Simulation results}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.


% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table. Table text will default to
% \footnotesize as IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that IEEE does not put floats in the very first column - or typically
% anywhere on the first page for that matter. Also, in-text middle ("here")
% positioning is not used. Most IEEE journals/conferences use top floats
% exclusively. Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the \fnbelowfloat
% command of the stfloats package.





%\subsection{BioTCMCloud}
%BioTCMCloud is a semantic data integration platform for the biomedical domain. It integrates most of the %typical biomedical ontologies across WM and TCM
%including Gene Ontology\footnote{http://www.geneontology.org}, Disease Ontology\footnote{http://www.disease-ontology.org}, Diseasome Ontology\footnote{http://www.diseasome.eu},
%DrugBank Ontology\footnote{http://www.drugbank.ca}, TCMGeneDit Ontology\footnote{http://code.google.com/p/junsbriefcase/wiki/RDFTCMData}, Uniprot Ontology\footnotemark[2],
%NCBI Gene Ontology\footnote{http://ncbi.nlm.nih.gov/gene} and so on. BioTCMCloud is also a data-as-a-service platform which allows complex data analytical queries across linked ontologies,
%mapping search, TCM inference search and so on. The knowledge base provides us with an ideal linked knowledge graph model which represents the big biomedical data cloud.
%\subsection{Distributed Reasoning Prototype System}


%\begin{theorem}
%In recent years, several Chinese herbs were found to exhibit a variety of effects through
%regulating a wide range of gene expressions or protein
%activities\cite{paper26}\cite{paper27}. Biomedical researchers are eager to discover associations between gene and herb to
%help understand the possible therapeutic mechanisms of TCMs via gene regulations.
%\end{theorem}


%The reasoning system is supposed to provide biomedical researchers with a knowledge discovery platform over the biomedical data cloud.

%So our reasoning system is implemented using rule based engines.

%the disease class connects to drug class with a rule "possibleDrug".
%B if we want to discover the associations between them, we need to create a reasoning rule set based on
%existing basic reasoning rules that can link them implicitly.

%The main purpose of our reasoning system is to provide biomedical researchers with a data analysis platform over biomedical data cloud. The association relationships between biomedical entities are expressed as association rules or reasoning rules in the reasoning system.
%So our reasoning system is implemented using rule based engines.
%Typically, some basic reasoning rules between biomedical entities are given directly by the biomedical ontology. For example, the drug class connects to disease class with a rule "Treat".But in most cases, there does not exist a direct association rule between two kinds of biomedical entities, such as Chinese Medical herb and gene. On this occasion, if we want to discover the associations between them, we need to create a reasoning rule set that can link the two kinds of biomedical entities implicitly based on existing basic reasoning rules. After determining the reasoning rule set, the reasoning system is responsible for deriving some new knowledge over the large-scale biomedical cloud.

%In particular, the reasoning system is supposed to solve the problem described as Problem 1:
%\begin{theorem}Suppose there is a instance triple graph G which represents millions of biomedical entities from biomedical data cloud. Given a reasoning rule set and two kinds of biomedical entities, the reasoning system is required to derive the implicit associations between them.
%derive new triples whose subject and object come from the two kinds of biomedical entities respectively and are linked by the reasoning rule set.\end{theorem}


%\emph{Definition 1:} \textbf{Reasoning Rule Chain (RRC)}.










%%\renewcommand{\algorithmicrequire}{\textbf{Initialization:}}
%%\renewcommand{\algorithmicensure}{\textbf{Iteration:}}
%%%正文代码
%\begin{algorithm}[htb]
%\caption{ Framework of OWL Reasoning Algorithm.}
%\label{alg:Framwork}
%\begin{algorithmic}[]
%\Require
%instance triple graph, $G_0$;
%Property Chain Set, $PCS_0$;
%two classes required to explore implicit semantic associations, Herb and Gene;
%number that has been iterated, $I=0$;
%number needed to be iterated, $M$;
%\Ensure
%\label{code:recentStart}
%\While {$I<M$}
%\State Step 1) Load triple graph and PCS on the current iteration, $G_I$, $PCS_I$;
%\State Step 2) Group instance triples based on join key;
%%\State   $//$ Corresponding to map function
%\State Step 3) Derive new instance triples;
%%\State   $//$ Corresponding to reduce function
%\State Step 4) Update input instance triple graph, $G$$_{I+1}$;
%\State Step 5) Update PCS, $PCS$$_{I+1}$;
%\State Step 6) $I\leftarrow I+1$;
%\EndWhile
%\label{code:recentEnd}
%
%\end{algorithmic}
%\end{algorithm}










%%
%\renewcommand{\algorithmicrequire}{\textbf{Map(key, value)}}   %改成后面的小标题
%\renewcommand{\algorithmicensure}{\textbf{Reduce(key, value)}}
%%%正文代码
%\begin{algorithm}[htb]
%\caption{ Naive OWL Reasoning Algorithm Based on MapReduce}
%\label{alg:naive}
%\begin{algorithmic}[]
%\Require
%\State $//$ key:linenumber(irrelevant)
%\State $//$ value:instance triple
%\State $PID$ = $PCS$.getIndex($triple$.predicate);
%\State $//$get the PID of the triple
%\If{$PID==0$}
%\State  $key$=$triple$.getObject();
%\ElsIf{$PID==1$}
%\State  $key$=$triple$.getSbject();
%\ElsIf{$1<PID<PCS.length-1$}
%\State  $key$=1;
%\Else
%\State return;
%\EndIf
%\State emit($key$, $value$);
%
%\Ensure
%\State $//$ key:join key
%\State $//$ value:triple
%\State $subjectList$ = empty;
%\State $objectList$ = empty;
%\State $PID$ = $PCS$.getIndex($triple$.predicate);
%\If{$key==1$}
%    \For{each $triple\in value$}
%        \State emit(null, $triple$);
%    \EndFor
%\Else
%    \For{each $triple\in value$}
%        \If{$PID==0$}
%         \State    $subjectList$.add($triple$.subject);
%        \Else
%         \State   $objectList$.add($triple$.object);
%
%     \EndIf
%     \EndFor
%
%     \State $new\_OPC$=$PCS[0] \bigotimes PCS[1]$;
%     \For{each $s\in subjectList$}
%       \For{each $o\in objectList$}
%        \State emit(null, triple($s$, $new\_OPC$, $o$));
%       \EndFor
%     \EndFor
%\EndIf
%\end{algorithmic}
%\end{algorithm}




%This rule guarantees that we can determine the join sequences in advance. Further more, according to this rule we can know exactly how to extract the join key from the triple.

%$\lfloor3.14\rfloor$, $\lceil3.14\rceil$



%In map function, we compute join key for every triple based on join conditions.
%The join key is used as intermediate key. Intermediate value is the triple itself.
%Each Map process outputs several pairs of intermediate results $<ik,iv>$.

%In reduce function, we firstly divide input triples into two classes based on the parity of triple's PID.
%If PID is odd, we extract triple's object to a set called ObjectList. Otherwise, we add triple's subject to
%the set called SubjectList. We are able to compute the Join Candidate Set based on ObjectList and SubjectList.
%Then we compute the shared OPC for all new derived triples. Subsequently, the output pairs $<ok,ov>$ are written to HDFS
%where ok is null and ov is the derived triples. The triples will form a new input triple graph for next iteration.



%\renewcommand{\algorithmicrequire}{\textbf{Map(key, value)}}   %改成后面的小标题
%\renewcommand{\algorithmicensure}{\textbf{Reduce(key, value)}}
%%%正文代码
%\begin{algorithm}[htb]
%\caption{ Efficient OWL Reasoning Algorithm Based on MapReduce}
%\label{alg:efficient}
%\begin{algorithmic}[]
%\Require
%\State $//$ key:linenumber(irrelevant)
%\State $//$ value:instance triple
%\State $PID$ = $PCS$.getPID($triple$.predicate);
%\If{$PID==-1$}
%\State return;
%\EndIf
%
%\If{$PID\%2==1$}
%\State  $key$=($PID$-1)+"\_"+$triple$.getSubject();
%\Else
%\State  $key$=$PID$+"\_"+$triple$.getObject();
%\EndIf
%\State emit($key$, $value$);
%
%\Ensure
%\State $//$ key:join key
%\State $//$ value:triple
%\State $subjectList$ = empty;
%\State $objectList$ = empty;
%\State $len$=PCS.length;
%\For{each $triple\in value$}
%    \State $PID$ = $PCS$.getPID($triple$.predicate);
%        \If{$PID ==(len-1)\&len\%2==1$}
%       % \State $//$ if the length of PCS is odd, these triples owning the largest PID are emitted directly.
%            \State emit(null, triple);
%            \State  return;
%         \EndIf
%
%        \If{$PID\%2==1$}
%         \State    $subjectList$.add($triple$.subject);
%        \Else
%         \State   $objectList$.add($triple$.object);
%        \EndIf
%\EndFor
%
%%\If{$PID\%2==1$}
% %    \State $new\_OPC$=$PCS[$PID$-1] \bigotimes PCS[$PID$]$;
%%\Else
% %     \State $new\_OPC$=$PCS[$PID$] \bigotimes PCS[$PID$+1]$;
% \State $new\_OPC$=ComputeOPC();
%
%
%     \For{each $s\in subjectList$}
%       \For{each $o\in objectList$}
%        \State emit(null, triple($s$,$new\_OPC$, $o$));
%       \EndFor
%     \EndFor
%%\EndIf
%\end{algorithmic}
%\end{algorithm}








%\subsection{Experimental Results and Discussion}
%\begin{table}[H]
%\caption{Evaluation of the associations between TCMs and genes}
%\setlength{\abovecaptionskip}{2pt}
%\setlength{\belowcaptionskip}{-2pt}
%\label{table_example}
%\begin{center}
%\begin{tabular}{|c|c|c|c|}
%\hline
%Gene Symbol & Sample Size & TP & Precision \\
%\hline
%TNF  & 30& 28& 93.3\%\\
%\hline
%PEP4  & 30& 22& 73.3\%\\
%\hline
%HK1  & 30& 24& 80\%\\
%\hline
%IL6 & 30& 26& 86.7\%\\
%\hline
%NQO1  & 30& 26& 86.7\%\\
%\hline
%Sum up & 150& 126& 84\%\\
%\hline
%\end{tabular}
%\end{center}
%\end{table}
%\begin{table}[H]
%\caption{Scalability over number of nodes}
%\label{table_example}
%\begin{center}
%\begin{tabular}{|c|c|c|}
%\hline
%Number of nodes & Time  (minutes)& Speedup\\
%\hline
%1 node & out of memory & \\
%\hline
%2 nodes & 34.65 & 1\\
%\hline
%4 nodes & 16.28 & 2.08 \\
%\hline
%6 nodes & 11.63 & 3.17\\
%\hline
%8 nodes & 8.81 &3.93\\
%\hline
%\end{tabular}
%\end{center}
%%\end{table}
%%\begin{table}[H]
%\caption{Scalability over input data}
%\label{table_example}
%\begin{center}
%\begin{tabular}{|c|c|c|}
%\hline
%Input Data (times) & Time  (minutes)& Sizeup\\
%\hline
%1 time & 8.81 & 1\\
%\hline
%2 times & 16.70 & 1.90 \\
%\hline
%3 times & 22.36 & 2.54\\
%\hline
%4 times & 30.51 &3.46\\
%\hline
%5 times & 46.39 &5.26\\
%\hline
%\end{tabular}
%\end{center}
%\end{table}











%MapReduce is a widely used programming model for big data processing on large clusters. OWL reasoning is a common approach to process the problems involved the high data correlation. To achieve higher performance, we have also presented some optimisations for reducing the running time of MapReduce. In our experiment, we have shown a scalable implementation of OWL reasoning over big biomedical data which results in some meaningful associations between Chinese herbs and Western Medical genes within a short time.

% conference papers do not normally have an appendix
% use section* for acknowledgement

% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)



%\bibliography{IEEEexample}



% that's all folks



